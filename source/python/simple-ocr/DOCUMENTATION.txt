Experiment: Digit extraction from containers using OpenCV
keywords: opencv knn 

The code is adapted from a sample OpenCV-based OCR engine: https://github.com/goncalopp/simple-ocr-opencv


Segmentation: applies adaptive image thresholding, find contours, finds bounding boxes for the contours, then prunes the boxes based on size, ratio, and other heuristics.
Feature Extraction: each feature is simply a 10x10 grayscale cutout of the segment in question.
Classification: applies KNN algorithm to find the nearest neighbor(s) among the training samples.

Training data:
A set of sampled digits generated by hand in various fonts. I picked fonts that look similar to the ones found on containers, though none are identical.

Sample command line arguments:
	black font on white/grey background:  --trainfile font_sample_digits_b font_sample_digits_b2 --dir=yard_images
	white font on grey/black background:  --trainfile font_sample_digits_w font_sample_digits_w3 --dir=yard_images
	troubleshoot segmentation example:    --trainfile font_sample_digits_w --verbose --file yard_images/20141121_104645.jpg


 
Results:
When training with just black fonts I can get digits to be recognized on white containers. Similarly, training with white fonts yielded some recognized digits on colored containers.
Accuracy is not good but is better than random (if trained on the correctly colored font.)
Segments are ok, not great.
I could never get the white-font and black-font training data to work well when combined. I could start to see it working when I only trained one or the other. I also had to use a small k (=1). No neighbors vote, only the best match is found.  Hypothesis is that the training data is too sparse.

Accuracy: low, not quantified

Pros:
The engine has a flexible pipeline that we could emulate. The segmentation, feature extraction, and classification phases are pluggable components, and the segmentation is a stack of composable, parameterized filters. So it is easy to experiment with different parameters or try a different set of filters.
The program has a training mode where it segments one or more input file(s), then asks the user for the ground truth of each segment.  (This is written out as a .box file. If the box file already exists it is simply read in instead.)

Cons:
The feature extraction phase is probably not how professionals do it. In the write up (here:http://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python) the author admits that this was a shortcut.
The training data is miniscule.



Further research:
Replace simplistic feature extraction with something more robust, possibly invariant to color, scale, and/or rotation.
Unskew images using container face detection.
Exponentially more training data.
* Try training with real-life data rather than font files. 
* Need training data that shows things that are not digits.
Prefix recognition
* Probably better to recognize full prefix, rather than individual letters, but this would require a new segmentation algorithm.
Segmentation improvements
* Find aligned, adjacent letter/number segments
* Fill in missing segments
Training improvements
* Allow manual adjustments to automatic segment bounding boxes
* Allow new segments to be accumulated on top of previously-trained data
** Purpose: establish ground truth for new segments without needing to re-enter old ones
Research best practices.
