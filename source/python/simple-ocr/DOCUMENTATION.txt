Experiment: Digit extraction from containers using OpenCV
keywords: opencv knn 

The code is adapted from a sample OpenCV-based OCR engine: https://github.com/goncalopp/simple-ocr-opencv


Segmentation: applies adaptive image thresholding, find contours, finds bounding boxes for the contours, then prunes the boxes based on size, ratio, and other heuristics.
Feature Extraction: each feature is simply a 10x10 grayscale cutout of the segment in question.
Classification: applies KNN algorithm to find the nearest neighbor(s) among the training samples.

Training data:
A set of sampled digits generated by hand in various fonts. I picked fonts that look similar to the ones found on containers, though none are identical.

Example command line arguments:
	black font on white/grey background:  --trainfile font_sample_chars_b font_sample_chars_b2 --dir=yard_images
	white font on grey/black background:  --trainfile font_sample_chars_w font_sample_chars_w2 --dir=yard_images
	verbose segmentation example:         --trainfile font_sample_chars_w --verbose --file yard_images/20141121_104645.jpg


 
Results:
1. When training with digits only, just black fonts, digits on white containers are identified fairly often.
Accuracy is NOT good but is better than random.
Segments are sometimes ok.
Training with white fonts doesn't seem to work at all, but the segmentation of the containers is so poor that the sample size is small.

2. Training with (black) letters and digits, digits are a little less reliable. Letters don't work well at all, although interestingly enough "JB Hunt" letters are partially recognized.

Pros:
The engine has a flexible pipeline that we could emulate. The segmentation, feature extraction, and classification phases are pluggable components, and the segmentation is a stack of composable, parameterized filters. So it is easy to experiment with different parameters or try a different set of filters.
The program has a training mode where it segments one or more input file(s), then asks the user for the ground truth of each segment.  (This is written out as a .box file. If the box file already exists it is simply read in instead.)

Issues:
The feature extraction phase is not how professionals do it. In the write up (here:http://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python) the author admits that this was a shortcut.
The training data is miniscule.
Colored containers aren't segmented properly. One possible cause: adaptive thresholding needs different settings, or needs to be inverted (play with the contours experiment to see what I mean.)


Further research:
Replace simplistic feature extraction with something more robust, ideally invariant to color, scale, and/or rotation.
Unskew images using container face detection.
Exponentially larger amount of training data.
* Try training with real-life data rather than font files. 
* Need negative training data, i.e. things that are not digits.
* Possibly better to train full prefixes, rather than individual letters, but this would require a new segmentation algorithm. Also, rare prefixes would not be recognized until trained.
Possible segmentation improvements:
* Handle colored containers
* Find aligned, adjacent letter/number segments
* Use erode/dilate to get rid of noise and merge adjacent segments. (Note: the order of these is dependent on the foreground color)
* Infer segments missing from a row or column
Possible training improvements
* Allow manual adjustments to automatically generated segments
* Allow new segments to be accumulated on top of previously-trained data
** Purpose: establish ground truth for new segments without needing to re-enter old ones
Research best practices.
